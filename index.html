<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.2"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Opticka: Opticka: Sensory Experiment Generator</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtreedata.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="extra.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectlogo"><img alt="Logo" src="opticka.png"/></td>
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Opticka<span id="projectnumber">&#160;2.04</span>
   </div>
   <div id="projectbrief">Opticka is a sensory experiment system for behavioral research.</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.2 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search",'Search','.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(document).ready(function(){initNavTree('index.html',''); initResizable(); });
/* @license-end */
</script>
<div id="doc-content">
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">Opticka: Sensory Experiment Generator </div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p ><a class="anchor" id="md_readme"></a> <a href="https://doi.org/10.5281/zenodo.592253"><img src="https://zenodo.org/badge/DOI/10.5281/zenodo.12293.svg" alt="DOI" style="pointer-events: none;" class="inline"/></a> <a href="https://open.vscode.dev/iandol/opticka"><img src="https://open.vscode.dev/badges/open-in-vscode.svg" alt="Open in Visual Studio Code" style="pointer-events: none;" class="inline"/></a></p>
<p >Opticka is an object oriented framework with optional GUI for the <a href="http://psychtoolbox.org/">Psychophysics toolbox (PTB)</a>, allowing full experimental presentation of complex visual or other stimuli. It is designed to work on Linux, macOS or Windows and interfaces via strobed words and ethernet for recording neurophysiological and behavioural data. Full behavioural control is available by use of a <a href="http://iandol.github.io/OptickaDocs/classstate_machine.html#details">Finite State-Machine</a> controller, in addition to simple method of constants (MOC) experiments. Opticka uses a TCP interface to both Eyelink &amp; Tobii Pro eyetrackers affording better control, reliability and data recording over using analog voltages alone. The various base classes can be used <em>without</em> the need to run the GUI (see <a href="http://iandol.github.io/OptickaDocs/optickatest.html"><code>optickatest.m</code></a> for an example), and plug-n-play stimuli provide a unified interface (setup, animate, draw, update, reset) to integrate into other PTB routines. The object methods take care of all the background geometry and normalisation, meaning stimuli are much easier to use than “raw” PTB commands alone. Analysis routines are also present for taking Plexon files (<code>.PL2</code> or <code>.PLX</code>), Eyelink files (<code>.EDF</code>), and behavioural responses and parsing them into a consistent structure, interfacing directly with <a href="http://fieldtrip.fcdonders.nl/start">Fieldtrip</a> for further spike, LFP, and spike-LFP analysis. Opticka is more modular and affords much better graphics control (most stimuli are optimised OpenGL) than <a href="http://www.brown.edu/Research/monkeylogic/">MonkeyLogic</a>. <br  />
</p>
<h1><a class="anchor" id="autotoc_md2"></a>
Example hardware setup</h1>
<p >The diagram below shows an example Opticka configuration setup:</p>
<p ><img src="https://github.com/iandol/opticka/raw/gh-pages/images/Opticka-Setup.png" alt="Example hardware setup to run Opticka" class="inline"/></p>
<p >GUI:</p>
<p ><img src="https://github.com/iandol/opticka/raw/gh-pages/images/opticka.png" alt="Opticka Screenshot" class="inline"/> <br  />
</p>
<h2><a class="anchor" id="autotoc_md3"></a>
Hardware currently supported:</h2>
<ul>
<li><b>Display + digital I/O</b>: high quality display (high bit depths, great colour management) and microsecond precise frame-locked digital I/O: <a href="https://www.crsltd.com/tools-for-vision-science/calibrated-displays/displaypp-lcd-monitor/">Display++ developed by CRS</a>.</li>
<li><b>Display + digital I/O</b>: high quality display (high bit depths) and microsecond precise digital I/O: <a href="http://vpixx.com/products/tools-for-vision-sciences/">DataPixx / ViewPixx / ProPixx</a>.</li>
<li><b>Display</b>: any normal monitor.</li>
<li><b>Digital I/O</b>: <a href="https://labjack.com/">LabJack</a> USB U3/U6 or T4/T7 DAQs, strobed words up to 12bits. The T4/T7 are preferred and work on all platforms.</li>
<li><b>Digital I/O</b>: [Arduino]() boards for simple TTL triggers for reward systems, MagStim etc.</li>
<li><b>Eyetracking</b>: [Eyelink 1000]() &ndash; uses the native ethernet link. This enables much better control, drawing stimuli and experiment values onto the eyelink screen. EDF files are stored and <code><a class="el" href="eyelink_analysis_8m.html">eyelinkAnalysis.m</a></code> uses native EDF loading for full trial-by-trial analysis without conversion.</li>
<li><b>Eyetracking</b>: [Tobii Pro Eyetrackers]() &ndash; uses the excellent <a href="https://github.com/dcnieho/Titta">Titta toolbox</a> to manage calibration and recording. Tobii Pro eyetrackers do not require head fixation.</li>
<li><b>Electrophysiology</b>: in theory any recording system that accepts digital triggers / strobed words, but I've only used Plexon Omniplex systems or EEG recording systems. Opticka can use TCP communication over ethernet to transmit current variable data to allow online data visualisation (PSTHs etc. for each experiment variable) on the Omniplex machine.</li>
<li><b>Photodiode boxes</b>: we prefer TSL251R light-to-voltage photodiodes, which can be recorded directy into your electrophysiology system or can generate digital triggers via an <a href="https://github.com/iandol/opticka/tree/master/tools/photodiode">Arduino interface</a>.</li>
</ul>
<h1><a class="anchor" id="autotoc_md4"></a>
Quick Documentation</h1>
<p ><code><a class="el" href="optickatest_8m.html">optickatest.m</a></code> is a minimal example showing a simple method of constants (MOC) experiment with 11 different animated stimuli varying across angle, contrast and orientation. Read the Matlab-generated documentation here: <a href="http://iandol.github.io/OptickaDocs/optickatest.html"><code>optickatest.m</code> Report</a>. More complex behavioural control (gaze-contingent experiments with variable logic per trial) utilises a state machine. You can see examples in the [CoreProtocols]() folder, these are loaded into the GUI but the state machine <code>.m</code> files show you the logic. <br  />
</p>
<p >There is also auto-generated class documentation here: <a href="http://iandol.github.io/OptickaDocs/inherits.html">Opticka Class Docs</a>, that details the major classes and their methods and properties. This is generated from the comments in the code, which as always could be improved... <br  />
</p>
<h1><a class="anchor" id="autotoc_md5"></a>
Install Instructions</h1>
<p >Opticka prefers the latest Psychophysics Toolbox (V3.0.17+) and at least Matlab 2017a (it uses object-oriented property validation introduced in that version). It has been tested and is mostly used on 64bit Ubuntu 20.04 &amp; macOS 10.15.x with Matlab 2021a. You can simply download the ZIP from Github, and add the contents/subdirectories to Matlab path (or run <code><a class="el" href="add_opticka_to_path_8m.html">addOptickaToPath.m</a></code> to do it for you). Or to keep easily up-to-date if you have git installed, clone this Github repo, CD to the folder then run run <code><a class="el" href="add_opticka_to_path_8m.html">addOptickaToPath.m</a></code>.</p>
<p >Opticka currently works on Linux, macOS and Windows, though the older LabJack U3/U6 interface currently only works under Linux and macOS (Labjack uses a different interface on Windows and Linux/macOS; the LabJack T4/T7 does work cross-platform however). Linux is <b>by far</b> the best OS according the PTB developer Mario Kleiner, and receives the majority of development work from him, therefore it is <em>strongly advised</em> to use it for experiments. My experience is that Linux is much more robust and performant than macOS or Windows, and it is well worth the effort to use Linux for PTB experimental computers.</p>
<h2><a class="anchor" id="autotoc_md6"></a>
Features</h2>
<ul>
<li>Values are always given in eye-relevant co-ordinates (degrees etc.) that are internally calculated based on screen geometry/distance</li>
<li>No limit on the number of independent variables, and variables can be linked to multiple stimuli.</li>
<li>A state machine logic can run behavioural tasks driven by for e.g. eye position or behavioural response. State machines can flexibly run tasks and chains of states define your experimental loop.</li>
<li>Number of heterogeneous stimuli displayed simultaneously only limited by the GPU / computer power.</li>
<li>Display lists are used, so one can easily change drawing order (i.e. what stimulus draws over other stimuli), by changing its order on the list.</li>
<li>Object Oriented, allowing stimulus classes to be easily added and code to autodocument using DOxygen.</li>
<li>The set of stimuli and variables can be saved into protocol files, to easily run successive protocols quickly.</li>
<li>Fairly comprehensive control of the PTB interface to the drawing hardware, like blending mode, bit depth, windowing, verbosity.</li>
<li>Colour is defined in floating point format, takes advantage of higher bit depths in newer graphics cards when available. The buffer can be defined from 8-32bits, use full alpha blending within that space and enable a &gt;8bit output using pseudogrey bitstealing techniques.</li>
<li>Sub-pixel precision (1/256th pixel) for movement and positioning.</li>
<li>TTL output to data acquisition and other devices. Currently uses DataPixx or LabJack to interface to the Plexon Omniplex using strobed words.</li>
<li>Can talk to other machines on the network during display using TCP/UDP (used to control a Plexon online display, so one can see PSTHs for each stimulus variable shown in real time).</li>
<li>Each stimulus has its own relative X &amp; Y position, and the screen centre can be arbitrarily moved via the GUI. This allows quick setup over particular parts of visual space, i.e. relative to a receptive field without needing to edit lots of other values.</li>
<li>Can record stimuli to video files.</li>
<li>Manages monitor calibration using ColorCalII or SpectroCalII from CRG or an i1Pro from ViewPixx. Calibration sets can be saved and loaded easily via the GUI.</li>
<li>Gratings (all using procedural textures for high performance):<ul>
<li>Per-frame update of properties for arbitrary numbers of grating patches.</li>
<li>Rectangular or circular aperture.</li>
<li>Cosine or hermite interpolation for filtering grating edges.</li>
<li>Square wave gratings, also using a procedural texture, i.e. very fast.</li>
<li>Gabors</li>
</ul>
</li>
<li>Coherent dot stimuli, coherence expressed from 0-1. Either square or round dots. Colours can be simple, random, random luminance or binary. Kill rates allow random replacement rates for dots. Circularly smoothed masked aperture option. Newsroom style dots with motion distributions etc.</li>
<li>Bars, either solid colour or checkerboard / random noise texture. Bars can be animated, direction can be independent of their angle.</li>
<li>Flashing/pulsing smoothed edge spots.</li>
<li>Pictures/Images that can drift and rotate.</li>
<li>Movies that can be scaled and drift. Movie playback is double-buffered to allow them to work alongside other stimuli.</li>
<li>Hand-mapping module - use mouse controlled dynamic bar / texture / colour to handmap receptive fields; includes logging of clicked position and later printout / storage of hand maps. These maps are in screen co-ordinates for quick subsequent stimulus placement. <br  />
 </li>
</ul>
</div></div><!-- PageDoc -->
</div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated on Wed Nov 3 2021 10:31:36 for Opticka by <a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.2 </li>
  </ul>
</div>
</body>
</html>
